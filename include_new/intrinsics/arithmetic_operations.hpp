/*
 * Boost Software License - Version 1.0 - August 17th, 2003
 *
 * Permission is hereby granted, free of charge, to any person or organization
 * obtaining a copy of the software and accompanying documentation covered by
 * this license (the "Software") to use, reproduce, display, distribute,
 * execute, and transmit the Software, and to prepare derivative works of the
 * Software, and to permit third-parties to whom the Software is furnished to
 * do so, all subject to the following:
 *
 * The copyright notices in the Software and this entire statement, including
 * the above license grant, this restriction and the following disclaimer,
 * must be included in all copies of the Software, in whole or in part, and
 * all derivative works of the Software, unless such copies or derivative
 * works are solely in the form of machine-executable object code generated by
 * a source language processor.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
 * SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
 * FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
 * DEALINGS IN THE SOFTWARE.
 */

/**
 * @file arithmetic_operations.hpp
 * @brief Operaciones aritméticas con carry/borrow multiplataforma
 *
 * Proporciona implementaciones optimizadas para operaciones aritméticas
 * con propagación de carry/borrow:
 * - GCC/Clang/Intel(Linux): __builtin_* cuando disponible
 * - MSVC/Intel(Windows): intrínsecos _addcarry_u64, _subborrow_u64
 * - Fallback: Implementación C++ puro portable
 *
 * @author Julián Calderón Almendros <julian.calderon.almendros@gmail.com>
 * @version 1.0.0
 * @date 2026-01-05
 * @copyright Boost Software License 1.0
 */

#ifndef INTRINSICS_ARITHMETIC_OPERATIONS_HPP
#define INTRINSICS_ARITHMETIC_OPERATIONS_HPP

#include "compiler_detection.hpp"
#include "fallback_portable.hpp"
#include <cstdint>

// Incluir <intrin.h> para MSVC y Intel ICX en Windows
#if INTRINSICS_USES_MSVC_ABI
#include <intrin.h>
#endif

namespace intrinsics
{

    // ============================================================================
    // ADDCARRY - Suma con carry
    // ============================================================================

    /**
     * @brief Suma dos uint64_t con carry de entrada y salida
     *
     * Realiza: result = a + b + carry_in, retorna carry_out
     *
     * @param carry_in Carry de entrada (0 o 1)
     * @param a Primer operando
     * @param b Segundo operando
     * @param result Puntero donde almacenar el resultado
     * @return Carry de salida (0 o 1)
     *
     * @note En tiempo de compilación usa implementación portable.
     *       En tiempo de ejecución usa intrínsecos optimizados.
     */
    inline constexpr unsigned char addcarry_u64(unsigned char carry_in, uint64_t a, uint64_t b,
                                                uint64_t *result) noexcept
    {
#if INTRINSICS_USES_MSVC_ABI
        // MSVC y Intel ICX en Windows: usar intrínsecos de MSVC
        if (INTRINSICS_IS_CONSTANT_EVALUATED())
        {
            // Versión constexpr portable
            uint64_t sum = a + b;
            uint64_t sum_with_carry = sum + carry_in;
            *result = sum_with_carry;
            // Detectar overflow: si sum < a (overflow en a+b) o sum_with_carry < sum (overflow en
            // +carry)
            return (sum < a) || (sum_with_carry < sum) ? 1 : 0;
        }
        else
        {
            return _addcarry_u64(carry_in, a, b, reinterpret_cast<unsigned long long *>(result));
        }
#elif INTRINSICS_USES_GNU_ABI
        // GCC/Clang/Intel(Linux): usar builtins
        if (INTRINSICS_IS_CONSTANT_EVALUATED())
        {
            // Versión constexpr portable
            uint64_t sum = a + b;
            uint64_t sum_with_carry = sum + carry_in;
            *result = sum_with_carry;
            return (sum < a) || (sum_with_carry < sum) ? 1 : 0;
        }
#if (defined(__GNUC__) && __GNUC__ >= 5) || (defined(__clang__) && __clang_major__ >= 3)
        // GCC/Clang: usar __builtin_uaddll_overflow para detección de overflow
        // Necesitamos manejar carry_in manualmente ya que __builtin_uaddll_overflow no lo soporta
        unsigned long long temp_sum;
        unsigned char overflow1 = __builtin_uaddll_overflow(a, b, &temp_sum);
        unsigned long long final_result;
        unsigned char overflow2 = __builtin_uaddll_overflow(temp_sum, carry_in, &final_result);
        *result = static_cast<uint64_t>(final_result);
        return overflow1 | overflow2;
#else
        // Fallback para versiones antiguas
        uint64_t sum = a + b;
        uint64_t sum_with_carry = sum + carry_in;
        *result = sum_with_carry;
        return (sum < a) || (sum_with_carry < sum) ? 1 : 0;
#endif
#else
        // Fallback portable
        uint64_t sum = a + b;
        uint64_t sum_with_carry = sum + carry_in;
        *result = sum_with_carry;
        return (sum < a) || (sum_with_carry < sum) ? 1 : 0;
#endif
    }

    /**
     * @brief Suma simple con carry (sin carry de entrada)
     *
     * Caso especial de addcarry_u64 con carry_in = 0
     * Útil para incrementos simples.
     *
     * @param a Primer operando
     * @param b Segundo operando
     * @param result Puntero donde almacenar el resultado
     * @return Carry de salida (0 o 1)
     */
    inline constexpr unsigned char add_u64(uint64_t a, uint64_t b, uint64_t *result) noexcept
    {
        return addcarry_u64(0, a, b, result);
    }

    // ============================================================================
    // SUBBORROW - Resta con borrow
    // ============================================================================

    /**
     * @brief Resta dos uint64_t con borrow de entrada y salida
     *
     * Realiza: result = a - b - borrow_in, retorna borrow_out
     *
     * @param borrow_in Borrow de entrada (0 o 1)
     * @param a Minuendo
     * @param b Sustraendo
     * @param result Puntero donde almacenar el resultado
     * @return Borrow de salida (0 o 1)
     *
     * @note En tiempo de compilación usa implementación portable.
     *       En tiempo de ejecución usa intrínsecos optimizados.
     */
    inline constexpr unsigned char subborrow_u64(unsigned char borrow_in, uint64_t a, uint64_t b,
                                                 uint64_t *result) noexcept
    {
#if INTRINSICS_USES_MSVC_ABI
        // MSVC y Intel ICX en Windows: usar intrínsecos de MSVC
        if (INTRINSICS_IS_CONSTANT_EVALUATED())
        {
            // Versión constexpr portable
            uint64_t diff = a - b;
            uint64_t diff_with_borrow = diff - borrow_in;
            *result = diff_with_borrow;
            // Detectar underflow
            return (diff > a) || (diff_with_borrow > diff) ? 1 : 0;
        }
        else
        {
            return _subborrow_u64(borrow_in, a, b, reinterpret_cast<unsigned long long *>(result));
        }
#else
        // GCC/Clang/Intel(Linux)/Fallback: usar implementación portable
        // NOTA: __builtin_subcll tiene comportamiento incorrecto en algunas versiones
        // Por eso usamos la versión portable que siempre funciona correctamente
        uint64_t diff = a - b;
        uint64_t diff_with_borrow = diff - borrow_in;
        *result = diff_with_borrow;
        return (diff > a) || (diff_with_borrow > diff) ? 1 : 0;
#endif
    }

    /**
     * @brief Resta simple con borrow (sin borrow de entrada)
     *
     * Caso especial de subborrow_u64 con borrow_in = 0
     *
     * @param a Minuendo
     * @param b Sustraendo
     * @param result Puntero donde almacenar el resultado
     * @return Borrow de salida (0 o 1)
     */
    inline constexpr unsigned char sub_u64(uint64_t a, uint64_t b, uint64_t *result) noexcept
    {
        return subborrow_u64(0, a, b, result);
    }

    // ============================================================================
    // MULX - Multiplicación extendida (produce 128 bits de resultado)
    // ============================================================================

    /**
     * @brief Multiplica dos uint64_t y produce resultado de 128 bits
     *
     * Realiza: (result_high:result_low) = a * b
     *
     * @param a Primer operando
     * @param b Segundo operando
     * @param result_high Puntero donde almacenar la parte alta (64 bits superiores)
     * @param result_low Puntero donde almacenar la parte baja (64 bits inferiores)
     *
     * @note Esta operación es fundamental para aritmética de precisión extendida.
     */
    inline constexpr void mulx_u64(uint64_t a, uint64_t b, uint64_t *result_high,
                                   uint64_t *result_low) noexcept
    {
#if INTRINSICS_USES_MSVC_ABI
        // MSVC y Intel ICX en Windows: usar _umul128
        if (INTRINSICS_IS_CONSTANT_EVALUATED())
        {
            // Versión constexpr: multiplicación de 64x64 -> 128 bits
            // Descomponer en partes de 32 bits
            uint64_t a_lo = static_cast<uint32_t>(a);
            uint64_t a_hi = a >> 32;
            uint64_t b_lo = static_cast<uint32_t>(b);
            uint64_t b_hi = b >> 32;

            uint64_t p0 = a_lo * b_lo;
            uint64_t p1 = a_lo * b_hi;
            uint64_t p2 = a_hi * b_lo;
            uint64_t p3 = a_hi * b_hi;

            uint64_t carry = ((p0 >> 32) + static_cast<uint32_t>(p1) + static_cast<uint32_t>(p2)) >> 32;

            *result_low = p0 + (p1 << 32) + (p2 << 32);
            *result_high = p3 + (p1 >> 32) + (p2 >> 32) + carry;
        }
        else
        {
            *result_low = _umul128(a, b, result_high);
        }
#elif defined(__SIZEOF_INT128__)
        // GCC/Clang/Intel(Linux) con soporte de __uint128_t
        __uint128_t product = static_cast<__uint128_t>(a) * static_cast<__uint128_t>(b);
        *result_low = static_cast<uint64_t>(product);
        *result_high = static_cast<uint64_t>(product >> 64);
#else
        // Fallback portable: multiplicación de 64x64 -> 128 bits
        uint64_t a_lo = static_cast<uint32_t>(a);
        uint64_t a_hi = a >> 32;
        uint64_t b_lo = static_cast<uint32_t>(b);
        uint64_t b_hi = b >> 32;

        uint64_t p0 = a_lo * b_lo;
        uint64_t p1 = a_lo * b_hi;
        uint64_t p2 = a_hi * b_lo;
        uint64_t p3 = a_hi * b_hi;

        uint64_t carry = ((p0 >> 32) + static_cast<uint32_t>(p1) + static_cast<uint32_t>(p2)) >> 32;

        *result_low = p0 + (p1 << 32) + (p2 << 32);
        *result_high = p3 + (p1 >> 32) + (p2 >> 32) + carry;
#endif
    }

    // ============================================================================
    // UMUL128 - Multiplicación extendida (retorna parte baja, alta por referencia)
    // ============================================================================

    /**
     * @brief Multiplica dos uint64_t, retorna parte baja y alta por referencia
     *
     * Similar a mulx_u64 pero con convención de retorno diferente
     * (compatible con _umul128 de MSVC)
     *
     * @param a Primer operando
     * @param b Segundo operando
     * @param result_high Puntero donde almacenar la parte alta
     * @return La parte baja del resultado (64 bits inferiores)
     */
    inline constexpr uint64_t umul128(uint64_t a, uint64_t b, uint64_t *result_high) noexcept
    {
        uint64_t result_low;
        mulx_u64(a, b, result_high, &result_low);
        return result_low;
    }

    // ============================================================================
    // UMULH - Multiplicación alta (solo parte alta de 64x64 -> 128)
    // ============================================================================

    /**
     * @brief Multiplica dos uint64_t y retorna solo la parte alta (64 bits superiores)
     *
     * Calcula (a * b) >> 64, útil para algoritmos de división/módulo de precisión extendida
     * como el Algoritmo D de Knuth.
     *
     * @param a Primer operando
     * @param b Segundo operando
     * @return Los 64 bits superiores del producto a × b
     *
     * @note Equivalente a __umulh de MSVC o ((__uint128_t)a * b) >> 64
     */
    inline constexpr uint64_t umulh(uint64_t a, uint64_t b) noexcept
    {
#if defined(__x86_64__) && defined(__BMI2__)
        // BMI2: usar _mulx_u64 (disponible desde Haswell 2013)
        if (!INTRINSICS_IS_CONSTANT_EVALUATED())
        {
            unsigned long long high_part;
            (void)_mulx_u64(a, b, &high_part);
            return high_part;
        }
#endif

#if defined(__SIZEOF_INT128__)
        // __uint128_t: portable y eficiente
        const __uint128_t result = static_cast<__uint128_t>(a) * b;
        return static_cast<uint64_t>(result >> 64);

#elif defined(__x86_64__) && (defined(__GNUC__) || defined(__clang__))
        // Inline assembly: mapeo directo a instrucción mulq
        if (!INTRINSICS_IS_CONSTANT_EVALUATED())
        {
            uint64_t high_part, low_part;
            __asm__("mulq %3" : "=a"(low_part), "=d"(high_part) : "a"(a), "r"(b) : "cc");
            return high_part;
        }
        // Fallthrough a implementación portable para constexpr
#endif

        // Fallback portable (funciona en constexpr)
        const uint32_t a_lo = static_cast<uint32_t>(a);
        const uint32_t a_hi = static_cast<uint32_t>(a >> 32);
        const uint32_t b_lo = static_cast<uint32_t>(b);
        const uint32_t b_hi = static_cast<uint32_t>(b >> 32);

        const uint64_t p0 = static_cast<uint64_t>(a_lo) * b_lo;
        const uint64_t p1 = static_cast<uint64_t>(a_lo) * b_hi;
        const uint64_t p2 = static_cast<uint64_t>(a_hi) * b_lo;
        const uint64_t p3 = static_cast<uint64_t>(a_hi) * b_hi;

        const uint64_t middle = p1 + (p0 >> 32) + static_cast<uint32_t>(p2);

        return p3 + (middle >> 32) + (p2 >> 32);
    }

    // ============================================================================
    // MUL128x64_HIGH - Parte alta de multiplicación 128x64 bits
    // ============================================================================

    /**
     * @brief Calcula la parte alta de uint128_t × uint64_t (bits 128-191 del resultado)
     *
     * Realiza: resultado = (hi:lo) × multiplier, retorna bits [128:191]
     * El resultado completo sería de 192 bits, esta función retorna los 64 bits superiores.
     *
     * @param lo Parte baja del multiplicando de 128 bits
     * @param hi Parte alta del multiplicando de 128 bits
     * @param multiplier Multiplicador de 64 bits
     * @return Los 64 bits superiores del producto de 192 bits
     *
     * @note Útil para el algoritmo de división de Knuth
     */
    inline constexpr uint64_t mul128x64_high(uint64_t lo, uint64_t hi, uint64_t multiplier) noexcept
    {
        // Calcular las tres partes de la multiplicación:
        // high_high = parte alta de hi * multiplier (bits 128-191)
        // low_high = parte alta de lo * multiplier (bits 64-127)
        // mid_low = parte baja de hi * multiplier (bits 64-127)
        const uint64_t high_high = umulh(hi, multiplier);
        const uint64_t low_high = umulh(lo, multiplier);
        const uint64_t mid_low = hi * multiplier;

        // Sumar las partes intermedias con detección de carry
        const uint64_t sum = low_high + mid_low;
        const uint64_t carry = (sum < low_high) ? 1 : 0;

        return high_high + carry;
    }

    // ============================================================================
    // KNUTH DIVISION HELPERS - Algoritmo D de Knuth
    // ============================================================================

    /**
     * @brief División nativa de 128 bits entre 64 bits (optimizada con __uint128_t)
     *
     * Calcula: quotient = numerator / divisor, remainder = numerator % divisor
     * Esta función utiliza la aritmética de 128 bits nativa cuando está disponible.
     *
     * @param num_hi Parte alta del numerador de 128 bits
     * @param num_lo Parte baja del numerador de 128 bits
     * @param divisor Divisor de 64 bits
     * @param[out] remainder Puntero donde se almacenará el resto (puede ser nullptr)
     * @return El cociente de la división
     *
     * @note Solo disponible con __uint128_t (GCC/Clang en x86-64/ARM64/RISC-V)
     * @note Intel ICX en Windows no soporta __udivti3/__umodti3, requiere fallback
     */
    inline constexpr uint64_t div128_64(uint64_t num_hi, uint64_t num_lo, uint64_t divisor,
                                        uint64_t *remainder = nullptr) noexcept
    {
#if defined(__SIZEOF_INT128__) && !defined(_MSC_VER)
        const __uint128_t numerator = (static_cast<__uint128_t>(num_hi) << 64) | num_lo;
        const __uint128_t q = numerator / divisor;
        if (remainder)
        {
            const __uint128_t r = numerator % divisor;
            *remainder = static_cast<uint64_t>(r);
        }
        return static_cast<uint64_t>(q);
#else
        // Fallback: esta función no debería llamarse sin __uint128_t
        // El caller debe verificar la disponibilidad antes de llamar
        (void)num_hi;
        (void)num_lo;
        (void)divisor;
        if (remainder)
            *remainder = 0;
        return 0;
#endif
    }

    /**
     * @brief Compone un dividendo de 128 bits desde resto alto + dato bajo y divide por 64 bits
     *
     * Esta función encapsula el "Paso 2" del algoritmo de división 128/64 bits:
     * Dados un resto alto (r_hi) de una división previa y la parte baja del dividendo original
     * (data_low), compone un nuevo dividendo de 128 bits ((r_hi << 64) | data_low) y lo divide por el
     * divisor.
     *
     * Algoritmo:
     *   1. Componer: dividend_composed = (r_hi << 64) | data_low
     *   2. Dividir: q_lo = dividend_composed / divisor
     *   3. Resto: remainder_final = dividend_composed % divisor
     *
     * @param r_hi Resto de la división previa (D1 % divisor) - 64 bits
     * @param data_low Parte baja del dividendo original (D0) - 64 bits
     * @param divisor Divisor de 64 bits
     * @param[out] remainder_final Puntero donde almacenar el resto final (obligatorio)
     * @return Cociente bajo (q_lo) de 64 bits
     *
     * @note Con __uint128_t: Usa división nativa optimizada (muy rápido)
     * @note Sin __uint128_t: Retorna 0 (el caller debe usar fallback iterativo)
     * @note Esta función es esencial para big_int con arrays de uint64_t mayores
     *
     * @example
     * @code
     * // Dividir 0x1'0000'0000'0000'0005 / 0x2
     * uint64_t D1 = 0x1, D0 = 0x5, divisor = 0x2;
     *
     * // Paso 1: D1 / divisor
     * uint64_t r_hi;
     * uint64_t q_hi = div128_64(0, D1, divisor, &r_hi); // q_hi=0, r_hi=1
     *
     * // Paso 2: Componer y dividir
     * uint64_t remainder_final;
     * uint64_t q_lo = div128_64_composed(r_hi, D0, divisor, &remainder_final);
     * // q_lo = 0x8000'0000'0000'0002, remainder_final = 1
     *
     * // Resultado: (q_hi << 64) | q_lo = 0x8000'0000'0000'0002, resto = 1
     * @endcode
     */
    inline constexpr uint64_t div128_64_composed(uint64_t r_hi, uint64_t data_low, uint64_t divisor,
                                                 uint64_t *remainder_final) noexcept
    {
#if defined(__SIZEOF_INT128__) && !defined(_MSC_VER)
        // Usar división nativa 128/64 con __uint128_t (optimizado)
        const __uint128_t dividend_composed = (static_cast<__uint128_t>(r_hi) << 64) | data_low;
        const uint64_t q_lo = static_cast<uint64_t>(dividend_composed / divisor);
        *remainder_final = static_cast<uint64_t>(dividend_composed % divisor);
        return q_lo;
#else
        // Sin __uint128_t: retornar 0 (el caller debe usar fallback iterativo)
        // Esto indica al caller que debe usar divrem(uint128_t) como fallback
        (void)r_hi;
        (void)data_low;
        (void)divisor;
        *remainder_final = 0;
        return 0;
#endif
    }

    /**
     * @brief Calcula q_hat para el algoritmo de división de Knuth
     *
     * Estima el cociente dividiendo los dos dígitos más significativos del dividendo
     * por el dígito más significativo del divisor, con refinamiento iterativo.
     *
     * @param u_ext Dígito de extensión del dividendo (bits 128-191)
     * @param u_hi Parte alta del dividendo normalizado (bits 64-127)
     * @param u_lo Parte baja del dividendo normalizado (bits 0-63)
     * @param v_hi Parte alta del divisor normalizado (bits 64-127)
     * @param v_lo Parte baja del divisor normalizado (bits 0-63)
     * @return El cociente estimado q_hat (entre 0 y 2^64-1)
     *
     * @note Parte del paso D3 del Algoritmo D de Knuth
     * @note Solo disponible con __uint128_t
     */
    inline constexpr uint64_t knuth_qhat(uint64_t u_ext, uint64_t u_hi, uint64_t u_lo, uint64_t v_hi,
                                         uint64_t v_lo) noexcept
    {
#if defined(__SIZEOF_INT128__) && !defined(_MSC_VER)
        // D3. Calcular q_hat (Estimación del cociente)
        const __uint128_t numerator = (static_cast<__uint128_t>(u_ext) << 64) | u_hi;

        // División nativa de 128/64 para estimar
        __uint128_t q_hat_wide = numerator / v_hi;
        __uint128_t r_hat_wide = numerator % v_hi;

        uint64_t q_hat = static_cast<uint64_t>(q_hat_wide);

        // Ajuste de q_hat: El bucle interno de Knuth para refinar la estimación
        while (true)
        {
            if (q_hat_wide >= 0xFFFFFFFFFFFFFFFFULL)
            {
                q_hat--;
                r_hat_wide += v_hi;
                if (r_hat_wide > 0xFFFFFFFFFFFFFFFFULL)
                    continue;
            }

            const __uint128_t left = static_cast<__uint128_t>(q_hat) * v_lo;
            const __uint128_t right = (r_hat_wide << 64) | u_lo;

            if (left > right)
            {
                q_hat--;
                r_hat_wide += v_hi;
                if (r_hat_wide > 0xFFFFFFFFFFFFFFFFULL)
                    break;
            }
            else
            {
                break;
            }
            q_hat_wide = q_hat;
        }

        return q_hat;
#else
        // Fallback: no debería llamarse sin __uint128_t
        (void)u_ext;
        (void)u_hi;
        (void)u_lo;
        (void)v_hi;
        (void)v_lo;
        return 0;
#endif
    }

    /**
     * @brief Ejecuta los pasos D4-D6 del algoritmo de Knuth: multiplicar, restar y corregir
     *
     * Multiplica q_hat × divisor, resta del dividendo, y corrige si el resultado es negativo.
     * Retorna el q_hat corregido (puede ser q_hat o q_hat-1).
     *
     * @param q_hat Cociente estimado
     * @param u_ext Extensión del dividendo (bits 128-191)
     * @param u_hi Parte alta del dividendo normalizado
     * @param u_lo Parte baja del dividendo normalizado
     * @param v_hi Parte alta del divisor normalizado
     * @param v_lo Parte baja del divisor normalizado
     * @param[out] remainder_hi Puntero donde se almacenará la parte alta del resto
     * @param[out] remainder_lo Puntero donde se almacenará la parte baja del resto
     * @return El cociente corregido (q_hat o q_hat-1)
     *
     * @note Parte de los pasos D4, D5 y D6 del Algoritmo D de Knuth
     */
    inline constexpr uint64_t knuth_multiply_subtract_correct(uint64_t q_hat, uint64_t u_ext,
                                                              uint64_t u_hi, uint64_t u_lo,
                                                              uint64_t v_hi, uint64_t v_lo,
                                                              uint64_t *remainder_hi,
                                                              uint64_t *remainder_lo) noexcept
    {
        // D4. Multiplicar y Restar: u -= q_hat * v
        // p_low = v * q_hat (parte baja de 128 bits)
        const uint64_t p_lo_lo = umul128(v_lo, q_hat, remainder_hi); // p_lo_lo y carry en remainder_hi
        const uint64_t carry_mid = *remainder_hi;
        const uint64_t p_lo_hi = v_hi * q_hat + carry_mid;

        // p_ext = parte alta de v * q_hat (bits 128-191)
        const uint64_t p_ext = mul128x64_high(v_lo, v_hi, q_hat);

        // Restar p_low de u_shifted
        const uint64_t old_u_lo = u_lo;
        uint64_t result_lo = u_lo - p_lo_lo;
        uint64_t borrow = (result_lo > old_u_lo) ? 1 : 0;

        const uint64_t old_u_hi = u_hi;
        uint64_t result_hi = u_hi - p_lo_hi - borrow;
        borrow = (result_hi > old_u_hi || (borrow && result_hi == old_u_hi)) ? 1 : 0;

        // Restar en la parte más significativa
        const int64_t final_balance =
            static_cast<int64_t>(u_ext) - static_cast<int64_t>(p_ext + borrow);

        // D5. Test de Resto (Corrección de signo)
        // D6. Add Back si es necesario
        uint64_t corrected_q = q_hat;
        if (final_balance < 0)
        {
            corrected_q--;
            // Devolver v al resto: result += v
            const uint64_t old_lo = result_lo;
            result_lo += v_lo;
            const uint64_t carry = (result_lo < old_lo) ? 1 : 0;
            result_hi += v_hi + carry;
        }

        *remainder_lo = result_lo;
        *remainder_hi = result_hi;
        return corrected_q;
    }

    /**
     * @brief Ejecuta el algoritmo completo de división de Knuth (pasos D3-D8)
     *
     * Realiza la división completa usando el algoritmo D de Knuth: estima q_hat,
     * multiplica, resta, corrige si es necesario y desnormaliza el resto.
     *
     * @param u_ext Extensión del dividendo normalizado (bits 128-191)
     * @param u_hi Parte alta del dividendo normalizado (bits 64-127)
     * @param u_lo Parte baja del dividendo normalizado (bits 0-63)
     * @param v_hi Parte alta del divisor normalizado (bits 64-127)
     * @param v_lo Parte baja del divisor normalizado (bits 0-63)
     * @param shift Cantidad de bits de normalización (para desnormalizar el resto)
     * @param[out] remainder_hi Puntero donde se almacenará la parte alta del resto desnormalizado
     * @param[out] remainder_lo Puntero donde se almacenará la parte baja del resto desnormalizado
     * @return El cociente de la división
     *
     * @note Implementa pasos D3-D8 del Algoritmo D de Knuth
     * @note Solo disponible con __uint128_t en plataformas no-Windows
     */
    inline constexpr uint64_t knuth_division_step(uint64_t u_ext, uint64_t u_hi, uint64_t u_lo,
                                                  uint64_t v_hi, uint64_t v_lo, int shift,
                                                  uint64_t *remainder_hi,
                                                  uint64_t *remainder_lo) noexcept
    {
#if defined(__SIZEOF_INT128__) && !defined(_MSC_VER)
        // D3. Calcular q_hat
        const uint64_t q_hat = knuth_qhat(u_ext, u_hi, u_lo, v_hi, v_lo);

        // D4-D6. Multiplicar, Restar y Corregir
        uint64_t result_hi, result_lo;
        const uint64_t corrected_q = knuth_multiply_subtract_correct(q_hat, u_ext, u_hi, u_lo, v_hi,
                                                                     v_lo, &result_hi, &result_lo);

        // D8. Desnormalizar Resto
        if (shift == 0)
        {
            *remainder_hi = result_hi;
            *remainder_lo = result_lo;
        }
        else
        {
            // Shift right: (result_hi:result_lo) >> shift
            *remainder_lo = (result_lo >> shift) | (result_hi << (64 - shift));
            *remainder_hi = result_hi >> shift;
        }

        return corrected_q;
#else
        (void)u_ext;
        (void)u_hi;
        (void)u_lo;
        (void)v_hi;
        (void)v_lo;
        (void)shift;
        *remainder_hi = 0;
        *remainder_lo = 0;
        return 0;
#endif
    }

    // ============================================================================
    // MUL128 - Multiplicación de 128 bits (retorna solo 128 bits bajos)
    // ============================================================================

    /**
     * @brief Multiplica dos números de 128 bits, retorna los 128 bits inferiores
     *
     * Realiza: (result_hi:result_lo) = (a_hi:a_lo) * (b_hi:b_lo) [128 bits bajos]
     *
     * @param a_lo Parte baja del primer operando (64 bits)
     * @param a_hi Parte alta del primer operando (64 bits)
     * @param b_lo Parte baja del segundo operando (64 bits)
     * @param b_hi Parte alta del segundo operando (64 bits)
     * @param result_lo Puntero donde almacenar los 64 bits bajos del resultado
     * @param result_hi Puntero donde almacenar los 64 bits altos del resultado
     *
     * @note Solo calcula los 128 bits inferiores del producto (trunca overflow).
     *       Para multiplicación 128x128 -> 256 bits se necesitaría otra función.
     */
    inline constexpr void mul128(uint64_t a_lo, uint64_t a_hi, uint64_t b_lo, uint64_t b_hi,
                                 uint64_t *result_lo, uint64_t *result_hi) noexcept
    {
#if defined(__SIZEOF_INT128__)
        // Compiladores con soporte nativo de 128 bits (GCC/Clang/Intel)
        __uint128_t a = (static_cast<__uint128_t>(a_hi) << 64) | a_lo;
        __uint128_t b = (static_cast<__uint128_t>(b_hi) << 64) | b_lo;
        __uint128_t result = a * b;
        *result_lo = static_cast<uint64_t>(result);
        *result_hi = static_cast<uint64_t>(result >> 64);
#else
        // Fallback manual compatible con constexpr (MSVC y contexto constexpr)
        // Multiplicación 64x64 -> 128 bits (parte baja: a_lo * b_lo)
        const uint64_t u1 = a_lo & 0xFFFFFFFF;
        const uint64_t v1 = b_lo & 0xFFFFFFFF;
        uint64_t t = u1 * v1;
        const uint64_t w3 = t & 0xFFFFFFFF;
        uint64_t k = t >> 32;

        const uint64_t u0 = a_lo >> 32;
        t = (u0 * v1) + k;
        k = t & 0xFFFFFFFF;
        const uint64_t w1 = t >> 32;

        const uint64_t v0 = b_lo >> 32;
        t = (u1 * v0) + k;
        k = t >> 32;

        const uint64_t lo_hi = (u0 * v0) + w1 + k;
        const uint64_t lo_lo = (t << 32) + w3;

        // Resultado final: lo_lo + (a_hi * b_lo + a_lo * b_hi + lo_hi) << 64
        *result_lo = lo_lo;
        *result_hi = (a_hi * b_lo) + (a_lo * b_hi) + lo_hi;
#endif
    }

} // namespace intrinsics

// ============================================================================
// NOTAS DE ARQUITECTURA
// ============================================================================

/**
 * @page arithmetic_ops_arch Notas de Arquitectura para Operaciones Aritméticas
 *
 * ## x86-64 (Intel/AMD)
 * - ADC: Add with carry (8086+)
 * - SBB: Subtract with borrow (8086+)
 * - MUL: Multiplicación sin signo produce resultado de doble tamaño
 * - MULX: Multiplicación extendida (BMI2, Haswell 2013+)
 * - Intrínsecos MSVC: _addcarry_u64, _subborrow_u64, _umul128
 *
 * ## ARM64 (AArch64)
 * - ADDS/ADCS: Add/Add with carry, actualiza flags
 * - SUBS/SBCS: Subtract/Subtract with carry, actualiza flags
 * - UMULH: Multiplicación alta sin signo (64x64 -> high 64)
 * - MUL: Multiplicación normal (64x64 -> low 64)
 *
 * ## ARM32 (AArch32)
 * - ADC: Add with carry
 * - SBC: Subtract with carry
 * - UMULL: Multiplicación larga sin signo (32x32 -> 64)
 *
 * ## RISC-V
 * - Sin instrucciones carry nativas en RV64I base
 * - Carry se simula comparando resultado con operandos
 * - Extensión M: MUL/MULH para multiplicación de precisión extendida
 *
 * ## Compiladores
 * - MSVC: _addcarry_u64, _subborrow_u64, _umul128 intrínsecos
 * - GCC 5+: __builtin_addcll, __builtin_subcll (constexpr en GCC 7+)
 * - Clang 3.8+: __builtin_addcll, __builtin_subcll
 * - Intel icpx: Soporta todos los builtins de GCC/Clang
 *
 * ## Evaluación en Tiempo de Compilación
 * - std::is_constant_evaluated() (C++20) o __builtin_is_constant_evaluated()
 * - Permite funciones constexpr que usan intrínsecos en runtime
 * - Fallback portable para constexpr context
 */

#endif // INTRINSICS_ARITHMETIC_OPERATIONS_HPP
